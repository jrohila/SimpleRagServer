services:
  opensearch:
    image: opensearchproject/opensearch:3.2.0
    environment:
      - discovery.type=single-node
      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=MyAdm1n_Passw0rd!
      - OPENSEARCH_JAVA_OPTS=-Xms1g -Xmx1g
      - plugins.security.ssl.http.enabled=false     # <-- allow plain HTTP on :9200
    ports:
      - "9200:9200"
      - "9600:9600"

  dashboards:
    image: opensearchproject/opensearch-dashboards:3.2.0
    environment:
      - OPENSEARCH_HOSTS=["http://opensearch:9200"] # stays http
    depends_on:
      - opensearch
    ports:
      - "${DASHBOARDS_PORT:-15601}:5601"  # 5601 falls in Windows excluded range 5555-5654; use 15601 by default

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    restart: unless-stopped
    profiles:
      - ollama

  ollama-pull-models:
    image: curlimages/curl:latest
    extra_hosts:
      - "host.docker.internal:host-gateway"  # ensure Linux can reach host; works on Windows/macOS too
    command: >
      sh -c '
        MODELS="$${OLLAMA_MODELS:-ibm/granite4:tiny-h embeddinggemma:300m}"
        RETRIES="$${OLLAMA_RETRY:-30}"
        DELAY="$${OLLAMA_RETRY_DELAY:-2}"
        OLLAMA_URL="$${OLLAMA_URL:-}"
        if [ -n "$$OLLAMA_URL" ]; then
          echo "Using provided OLLAMA_URL=$$OLLAMA_URL"
        else
          echo "Locating Ollama (container or host)..."
          for U in http://ollama:11434 http://host.docker.internal:11434; do
            echo "Trying $$U ..."
            if curl -sS -m 2 "$$U/api/tags" >/dev/null 2>&1 || \
               curl -sS --retry "$$RETRIES" --retry-delay "$$DELAY" --retry-all-errors -m 2 "$$U/api/tags" >/dev/null 2>&1; then
              OLLAMA_URL="$$U"
              break
            fi
          done
        fi
        if [ -z "$$OLLAMA_URL" ]; then
          echo "Ollama not reachable; skipping model pulls."
          exit 0
        fi
        echo "Using $$OLLAMA_URL"
        for M in $$MODELS; do
          echo "Pulling $$M..."
          curl -sS -X POST "$$OLLAMA_URL/api/pull" -H "Content-Type: application/json" -d "{\"name\":\"$$M\"}" || true
          echo
        done
        echo "Available models:"
        curl -sS "$$OLLAMA_URL/api/tags" || true
      '
    restart: "no"

  docling-serve:
    image: quay.io/docling-project/docling-serve:latest
    ports:
      - "5001:5001"
    environment:
      - DOCLING_SERVE_ENABLE_UI=1
      - DOCLING_SERVE_MAX_SYNC_WAIT=600
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s


  text-generation-inference:
    image: ghcr.io/huggingface/text-generation-inference:latest
    ports:
      - "8000:80"
    environment:
      - MODEL_ID=t5-large
      # - NUM_SHARD=1  # Uncomment for multi-GPU
      # - MAX_INPUT_LENGTH=1024  # Optional: limit input size
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

volumes:
  ollama_data:
